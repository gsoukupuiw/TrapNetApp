{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a45b73c-94c7-4766-b5e5-6f40d1a8f2c0",
   "metadata": {},
   "source": [
    "## Install Required Libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2f8c9889-18f5-44e6-8339-356f85ad1696",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /opt/conda/envs/pytorch/lib/python3.10/site-packages (4.40.1)\n",
      "Requirement already satisfied: torch in /opt/conda/envs/pytorch/lib/python3.10/site-packages (1.13.1)\n",
      "Requirement already satisfied: google-cloud-storage in /opt/conda/envs/pytorch/lib/python3.10/site-packages (2.14.0)\n",
      "Requirement already satisfied: google-api-python-client in /opt/conda/envs/pytorch/lib/python3.10/site-packages (1.8.0)\n",
      "Requirement already satisfied: google-cloud-aiplatform in ./.local/lib/python3.10/site-packages (1.49.0)\n",
      "Requirement already satisfied: tqdm in /opt/conda/envs/pytorch/lib/python3.10/site-packages (4.66.2)\n",
      "Requirement already satisfied: filelock in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from transformers) (3.13.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in ./.local/lib/python3.10/site-packages (from transformers) (0.22.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from transformers) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from transformers) (2024.4.28)\n",
      "Requirement already satisfied: requests in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from transformers) (0.4.3)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch) (11.7.99)\n",
      "Requirement already satisfied: setuptools in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (69.5.1)\n",
      "Requirement already satisfied: wheel in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (0.43.0)\n",
      "Requirement already satisfied: google-auth<3.0dev,>=2.23.3 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from google-cloud-storage) (2.29.0)\n",
      "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from google-cloud-storage) (1.34.1)\n",
      "Requirement already satisfied: google-cloud-core<3.0dev,>=2.3.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from google-cloud-storage) (2.4.1)\n",
      "Requirement already satisfied: google-resumable-media>=2.6.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from google-cloud-storage) (2.7.0)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from google-cloud-storage) (1.5.0)\n",
      "Requirement already satisfied: httplib2<1dev,>=0.9.2 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from google-api-python-client) (0.22.0)\n",
      "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from google-api-python-client) (0.2.0)\n",
      "Requirement already satisfied: six<2dev,>=1.6.1 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from google-api-python-client) (1.16.0)\n",
      "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from google-api-python-client) (3.0.1)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from google-cloud-aiplatform) (1.23.0)\n",
      "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from google-cloud-aiplatform) (3.20.3)\n",
      "Requirement already satisfied: google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0 in ./.local/lib/python3.10/site-packages (from google-cloud-aiplatform) (3.21.0)\n",
      "Requirement already satisfied: google-cloud-resource-manager<3.0.0dev,>=1.3.3 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from google-cloud-aiplatform) (1.12.3)\n",
      "Requirement already satisfied: shapely<3.0.0dev in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from google-cloud-aiplatform) (2.0.4)\n",
      "Requirement already satisfied: pydantic<3 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from google-cloud-aiplatform) (2.7.0)\n",
      "Requirement already satisfied: docstring-parser<1 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from google-cloud-aiplatform) (0.16)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-storage) (1.63.0)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform) (1.62.1)\n",
      "Requirement already satisfied: grpcio-status<2.0dev,>=1.33.2 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform) (1.48.2)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from google-auth<3.0dev,>=2.23.3->google-cloud-storage) (5.3.3)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from google-auth<3.0dev,>=2.23.3->google-cloud-storage) (0.4.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from google-auth<3.0dev,>=2.23.3->google-cloud-storage) (4.9)\n",
      "Requirement already satisfied: python-dateutil<3.0dev,>=2.7.2 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0->google-cloud-aiplatform) (2.9.0)\n",
      "Requirement already satisfied: grpc-google-iam-v1<1.0.0dev,>=0.12.4 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from google-cloud-resource-manager<3.0.0dev,>=1.3.3->google-cloud-aiplatform) (0.13.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from httplib2<1dev,>=0.9.2->google-api-python-client) (3.1.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2024.3.1)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from pydantic<3->google-cloud-aiplatform) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.1 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from pydantic<3->google-cloud-aiplatform) (2.18.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from requests->transformers) (2024.2.2)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0dev,>=2.23.3->google-cloud-storage) (0.6.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers torch google-cloud-storage google-api-python-client google-cloud-aiplatform tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d120ca-b6e3-4ee9-8fe1-8658c7b8fafb",
   "metadata": {},
   "source": [
    "## Download the Hugging Face Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a64026b8-34e6-4591-86b9-30f4af4b6166",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# Specify the Hugging Face model ID\n",
    "model_id = \"ealvaradob/bert-finetuned-phishing\"\n",
    "\n",
    "# Download and save the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# Download and save the model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_id)\n",
    "\n",
    "# Save the tokenizer and model to the local directory\n",
    "tokenizer.save_pretrained(\"./huggingface_model\")\n",
    "model.save_pretrained(\"./huggingface_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e84a4a-d414-4bdf-8588-80df5fa1e45c",
   "metadata": {},
   "source": [
    "## Convert to tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6aa795e5-52e4-406e-b3ea-7af76f1bc263",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "\nTFAutoModelForSequenceClassification requires the TensorFlow library but it was not found in your environment.\nHowever, we were able to find a PyTorch installation. PyTorch classes do not begin\nwith \"TF\", but are otherwise identically named to our TF classes.\nIf you want to use PyTorch, please use those classes instead!\n\nIf you really do want to use TensorFlow, please follow the instructions on the\ninstallation page https://www.tensorflow.org/install that match your environment.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m model_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mealvaradob/bert-finetuned-phishing\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      5\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_id)\n\u001b[0;32m----> 6\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mTFAutoModelForSequenceClassification\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m(model_id)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Save the model as a TensorFlow SavedModel\u001b[39;00m\n\u001b[1;32m      9\u001b[0m model\u001b[38;5;241m.\u001b[39msave_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./huggingface_model/tf_model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py:1450\u001b[0m, in \u001b[0;36mDummyObject.__getattribute__\u001b[0;34m(cls, key)\u001b[0m\n\u001b[1;32m   1448\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m key \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_from_config\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1449\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(key)\n\u001b[0;32m-> 1450\u001b[0m \u001b[43mrequires_backends\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backends\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/transformers/utils/import_utils.py:1433\u001b[0m, in \u001b[0;36mrequires_backends\u001b[0;34m(obj, backends)\u001b[0m\n\u001b[1;32m   1431\u001b[0m \u001b[38;5;66;03m# Raise the inverse error for PyTorch users trying to load TF classes\u001b[39;00m\n\u001b[1;32m   1432\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m backends \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m backends \u001b[38;5;129;01mand\u001b[39;00m is_torch_available() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_tf_available():\n\u001b[0;32m-> 1433\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(TF_IMPORT_ERROR_WITH_PYTORCH\u001b[38;5;241m.\u001b[39mformat(name))\n\u001b[1;32m   1435\u001b[0m checks \u001b[38;5;241m=\u001b[39m (BACKENDS_MAPPING[backend] \u001b[38;5;28;01mfor\u001b[39;00m backend \u001b[38;5;129;01min\u001b[39;00m backends)\n\u001b[1;32m   1436\u001b[0m failed \u001b[38;5;241m=\u001b[39m [msg\u001b[38;5;241m.\u001b[39mformat(name) \u001b[38;5;28;01mfor\u001b[39;00m available, msg \u001b[38;5;129;01min\u001b[39;00m checks \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m available()]\n",
      "\u001b[0;31mImportError\u001b[0m: \nTFAutoModelForSequenceClassification requires the TensorFlow library but it was not found in your environment.\nHowever, we were able to find a PyTorch installation. PyTorch classes do not begin\nwith \"TF\", but are otherwise identically named to our TF classes.\nIf you want to use PyTorch, please use those classes instead!\n\nIf you really do want to use TensorFlow, please follow the instructions on the\ninstallation page https://www.tensorflow.org/install that match your environment.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFAutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "# Load the Hugging Face model and tokenizer\n",
    "model_id = \"ealvaradob/bert-finetuned-phishing\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(model_id)\n",
    "\n",
    "# Save the model as a TensorFlow SavedModel\n",
    "model.save_pretrained(\"./huggingface_model/tf_model\")\n",
    "tokenizer.save_pretrained(\"./huggingface_model/tf_model\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb8b1cc-ce20-45e8-a0e2-4e34a0eb56ac",
   "metadata": {},
   "source": [
    "## Compress the Model Files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3038401f-18e7-4bc4-997c-0ac3a6d7ace4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compressing: 100%|██████████| 6/6 [01:17<00:00, 12.83s/file]\n"
     ]
    }
   ],
   "source": [
    "import os, tarfile\n",
    "\n",
    "def compress_folder(input_dir, output_file):\n",
    "    with tarfile.open(output_file, \"w:gz\") as tar:\n",
    "        for root, _, files in os.walk(input_dir):\n",
    "            for file in files:\n",
    "                tar.add(os.path.join(root, file), arcname=os.path.relpath(os.path.join(root, file), input_dir))\n",
    "\n",
    "compress_folder(\"./huggingface_model/tf_model\", \"model.tar.gz\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb348567-fcd3-4aff-a5f4-c7d89049f848",
   "metadata": {},
   "source": [
    "## Set Up a Google Cloud Project and Bucket:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84b8aa3c-8c39-4692-b6ef-ce4cb328ecc7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated property [core/project].\n",
      "Creating gs://tidy-reporter-model-bucket/...\n",
      "ServiceException: 409 A Cloud Storage bucket named 'tidy-reporter-model-bucket' already exists. Try another name. Bucket names must be globally unique across all Google Cloud projects, including those outside of your organization.\n"
     ]
    }
   ],
   "source": [
    "!gcloud config set project tidy-reporter-417118\n",
    "!gsutil mb gs://tidy-reporter-model-bucket/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b28f6f77-36fd-469e-99f9-a25607151072",
   "metadata": {},
   "source": [
    "## Upload the Model to Google Cloud Storage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "befb5696-ac74-4a56-a288-14af72379b4c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file://model.tar.gz [Content-Type=application/x-tar]...\n",
      "==> NOTE: You are uploading one or more large file(s), which would run          \n",
      "significantly faster if you enable parallel composite uploads. This\n",
      "feature can be enabled by editing the\n",
      "\"parallel_composite_upload_threshold\" value in your .boto\n",
      "configuration file. However, note that if you do this large files will\n",
      "be uploaded as `composite objects\n",
      "<https://cloud.google.com/storage/docs/composite-objects>`_,which\n",
      "means that any user who downloads such objects will need to have a\n",
      "compiled crcmod installed (see \"gsutil help crcmod\"). This is because\n",
      "without a compiled crcmod, computing checksums on composite objects is\n",
      "so slow that gsutil disables downloads of composite objects.\n",
      "\n",
      "- [1 files][  1.2 GiB/  1.2 GiB]                                                \n",
      "Operation completed over 1 objects/1.2 GiB.                                      \n",
      "Updated ACL on gs://tidy-reporter-model-bucket/model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "!gsutil cp model.tar.gz gs://tidy-reporter-model-bucket/\n",
    "!gsutil acl ch -u AllUsers:R gs://tidy-reporter-model-bucket/model.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a06b3911-9d80-45b0-ba0d-36c5867cbdc0",
   "metadata": {},
   "source": [
    "## Initialize the Vertex AI SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "683d14a1-35c4-4f4d-b82e-1dafa5e7b13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud services enable aiplatform.googleapis.com\n",
    "from google.cloud import aiplatform\n",
    "\n",
    "aiplatform.init(\n",
    "    project=\"tidy-reporter-417118\",\n",
    "    location=\"us-central1\",\n",
    "    staging_bucket=\"gs://tidy-reporter-model-bucket/\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5464f4e7-bebe-4924-ae41-69f7466d6162",
   "metadata": {},
   "source": [
    "## Create and Deploy the Model with Vertex AI:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "df472eda-4dd0-45c0-8660-223f964ef1e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /opt/conda/envs/pytorch/lib/python3.10/site-packages (4.40.1)\n",
      "Requirement already satisfied: torch in /opt/conda/envs/pytorch/lib/python3.10/site-packages (1.13.1)\n",
      "Requirement already satisfied: google-cloud-aiplatform in ./.local/lib/python3.10/site-packages (1.49.0)\n",
      "Requirement already satisfied: tqdm in /opt/conda/envs/pytorch/lib/python3.10/site-packages (4.66.2)\n",
      "Requirement already satisfied: git-lfs in /opt/conda/envs/pytorch/lib/python3.10/site-packages (1.6)\n",
      "Requirement already satisfied: filelock in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from transformers) (3.13.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in ./.local/lib/python3.10/site-packages (from transformers) (0.22.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from transformers) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from transformers) (2024.4.28)\n",
      "Requirement already satisfied: requests in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from transformers) (0.4.3)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from torch) (11.7.99)\n",
      "Requirement already satisfied: setuptools in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (69.5.1)\n",
      "Requirement already satisfied: wheel in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch) (0.43.0)\n",
      "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform) (1.34.1)\n",
      "Requirement already satisfied: google-auth<3.0.0dev,>=2.14.1 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from google-cloud-aiplatform) (2.29.0)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from google-cloud-aiplatform) (1.23.0)\n",
      "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from google-cloud-aiplatform) (3.20.3)\n",
      "Requirement already satisfied: google-cloud-storage<3.0.0dev,>=1.32.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from google-cloud-aiplatform) (2.14.0)\n",
      "Requirement already satisfied: google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0 in ./.local/lib/python3.10/site-packages (from google-cloud-aiplatform) (3.21.0)\n",
      "Requirement already satisfied: google-cloud-resource-manager<3.0.0dev,>=1.3.3 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from google-cloud-aiplatform) (1.12.3)\n",
      "Requirement already satisfied: shapely<3.0.0dev in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from google-cloud-aiplatform) (2.0.4)\n",
      "Requirement already satisfied: pydantic<3 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from google-cloud-aiplatform) (2.7.0)\n",
      "Requirement already satisfied: docstring-parser<1 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from google-cloud-aiplatform) (0.16)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform) (1.63.0)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform) (1.62.1)\n",
      "Requirement already satisfied: grpcio-status<2.0dev,>=1.33.2 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform) (1.48.2)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform) (5.3.3)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform) (0.4.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform) (4.9)\n",
      "Requirement already satisfied: google-cloud-core<3.0.0dev,>=1.6.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0->google-cloud-aiplatform) (2.4.1)\n",
      "Requirement already satisfied: google-resumable-media<3.0dev,>=0.6.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0->google-cloud-aiplatform) (2.7.0)\n",
      "Requirement already satisfied: python-dateutil<3.0dev,>=2.7.2 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0->google-cloud-aiplatform) (2.9.0)\n",
      "Requirement already satisfied: grpc-google-iam-v1<1.0.0dev,>=0.12.4 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from google-cloud-resource-manager<3.0.0dev,>=1.3.3->google-cloud-aiplatform) (0.13.0)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from google-cloud-storage<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (1.5.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2024.3.1)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from pydantic<3->google-cloud-aiplatform) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.1 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from pydantic<3->google-cloud-aiplatform) (2.18.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from requests->transformers) (2024.2.2)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform) (0.6.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/envs/pytorch/lib/python3.10/site-packages (from python-dateutil<3.0dev,>=2.7.2->google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0->google-cloud-aiplatform) (1.16.0)\n",
      "Updated property [core/project].\n",
      "\u001b[1;33mWARNING:\u001b[0m Your config file at [/home/jupyter/.docker/config.json] contains these credential helper entries:\n",
      "\n",
      "{\n",
      "  \"credHelpers\": {\n",
      "    \"gcr.io\": \"gcloud\",\n",
      "    \"us.gcr.io\": \"gcloud\",\n",
      "    \"eu.gcr.io\": \"gcloud\",\n",
      "    \"asia.gcr.io\": \"gcloud\",\n",
      "    \"staging-k8s.gcr.io\": \"gcloud\",\n",
      "    \"marketplace.gcr.io\": \"gcloud\",\n",
      "    \"us-central1-docker.pkg.dev\": \"gcloud\"\n",
      "  }\n",
      "}\n",
      "Adding credentials for: us-central1-docker.pkg.dev\n",
      "gcloud credential helpers already registered correctly.\n",
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "git-lfs is already the newest version (2.13.2-1+b5).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 6 not upgraded.\n",
      "Git LFS initialized.\n",
      "Cloning into 'huggingface_model'...\n",
      "remote: Enumerating objects: 139, done.\u001b[K\n",
      "remote: Counting objects: 100% (135/135), done.\u001b[K\n",
      "remote: Compressing objects: 100% (135/135), done.\u001b[K\n",
      "remote: Total 139 (delta 48), reused 0 (delta 0), pack-reused 4 (from 1)\u001b[K\n",
      "Receiving objects: 100% (139/139), 332.70 KiB | 4.26 MiB/s, done.\n",
      "Resolving deltas: 100% (48/48), done.\n",
      "Sending build context to Docker daemon  1.371MB\n",
      "Step 1/6 : FROM python:3.8-slim\n",
      " ---> 5b29acce18dd\n",
      "Step 2/6 : WORKDIR /\n",
      " ---> Using cache\n",
      " ---> d2c29b2fb1ec\n",
      "Step 3/6 : COPY . /\n",
      " ---> 8b0f8efaac9b\n",
      "Step 4/6 : RUN pip install transformers==4.38.1 torch==2.2.0\n",
      " ---> Running in 422e60722f89\n",
      "Collecting transformers==4.38.1\n",
      "  Downloading transformers-4.38.1-py3-none-any.whl (8.5 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 8.5/8.5 MB 29.9 MB/s eta 0:00:00\n",
      "Collecting torch==2.2.0\n",
      "  Downloading torch-2.2.0-cp38-cp38-manylinux1_x86_64.whl (755.5 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 755.5/755.5 MB 1.8 MB/s eta 0:00:00\n",
      "Collecting filelock\n",
      "  Downloading filelock-3.14.0-py3-none-any.whl (12 kB)\n",
      "Collecting tqdm>=4.27\n",
      "  Downloading tqdm-4.66.4-py3-none-any.whl (78 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 78.3/78.3 kB 14.9 MB/s eta 0:00:00\n",
      "Collecting regex!=2019.12.17\n",
      "  Downloading regex-2024.4.28-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (777 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 777.1/777.1 kB 64.7 MB/s eta 0:00:00\n",
      "Collecting requests\n",
      "  Downloading requests-2.31.0-py3-none-any.whl (62 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 62.6/62.6 kB 3.3 MB/s eta 0:00:00\n",
      "Collecting tokenizers<0.19,>=0.14\n",
      "  Downloading tokenizers-0.15.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 3.6/3.6 MB 74.5 MB/s eta 0:00:00\n",
      "Collecting numpy>=1.17\n",
      "  Downloading numpy-1.24.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 17.3/17.3 MB 70.6 MB/s eta 0:00:00\n",
      "Collecting huggingface-hub<1.0,>=0.19.3\n",
      "  Downloading huggingface_hub-0.23.0-py3-none-any.whl (401 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 401.2/401.2 kB 48.3 MB/s eta 0:00:00\n",
      "Collecting safetensors>=0.4.1\n",
      "  Downloading safetensors-0.4.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.2/1.2 MB 75.4 MB/s eta 0:00:00\n",
      "Collecting packaging>=20.0\n",
      "  Downloading packaging-24.0-py3-none-any.whl (53 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 53.5/53.5 kB 9.4 MB/s eta 0:00:00\n",
      "Collecting pyyaml>=5.1\n",
      "  Downloading PyYAML-6.0.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (736 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 736.6/736.6 kB 66.6 MB/s eta 0:00:00\n",
      "Collecting nvidia-curand-cu12==10.3.2.106\n",
      "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 56.5/56.5 MB 28.8 MB/s eta 0:00:00\n",
      "Collecting nvidia-cuda-runtime-cu12==12.1.105\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 823.6/823.6 kB 70.3 MB/s eta 0:00:00\n",
      "Collecting fsspec\n",
      "  Downloading fsspec-2024.3.1-py3-none-any.whl (171 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 172.0/172.0 kB 28.4 MB/s eta 0:00:00\n",
      "Collecting nvidia-cuda-cupti-cu12==12.1.105\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 14.1/14.1 MB 73.3 MB/s eta 0:00:00\n",
      "Collecting triton==2.2.0\n",
      "  Downloading triton-2.2.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (167.9 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 167.9/167.9 MB 10.3 MB/s eta 0:00:00\n",
      "Collecting sympy\n",
      "  Downloading sympy-1.12-py3-none-any.whl (5.7 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.7/5.7 MB 72.3 MB/s eta 0:00:00\n",
      "Collecting jinja2\n",
      "  Downloading Jinja2-3.1.3-py3-none-any.whl (133 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 133.2/133.2 kB 24.2 MB/s eta 0:00:00\n",
      "Collecting networkx\n",
      "  Downloading networkx-3.1-py3-none-any.whl (2.1 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.1/2.1 MB 68.4 MB/s eta 0:00:00\n",
      "Collecting nvidia-cublas-cu12==12.1.3.1\n",
      "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 410.6/410.6 MB 3.6 MB/s eta 0:00:00\n",
      "Collecting nvidia-cusolver-cu12==11.4.5.107\n",
      "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 124.2/124.2 MB 6.1 MB/s eta 0:00:00\n",
      "Collecting nvidia-cusparse-cu12==12.1.0.106\n",
      "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 196.0/196.0 MB 7.9 MB/s eta 0:00:00\n",
      "Collecting nvidia-cudnn-cu12==8.9.2.26\n",
      "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 731.7/731.7 MB 1.8 MB/s eta 0:00:00\n",
      "Collecting nvidia-nccl-cu12==2.19.3\n",
      "  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 166.0/166.0 MB 5.2 MB/s eta 0:00:00\n",
      "Collecting nvidia-nvtx-cu12==12.1.105\n",
      "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 99.1/99.1 kB 639.5 kB/s eta 0:00:00\n",
      "Collecting nvidia-cufft-cu12==11.0.2.54\n",
      "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 121.6/121.6 MB 6.5 MB/s eta 0:00:00\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.1.105\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 23.7/23.7 MB 21.0 MB/s eta 0:00:00\n",
      "Collecting typing-extensions>=4.8.0\n",
      "  Downloading typing_extensions-4.11.0-py3-none-any.whl (34 kB)\n",
      "Collecting nvidia-nvjitlink-cu12\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 21.1/21.1 MB 23.7 MB/s eta 0:00:00\n",
      "Collecting MarkupSafe>=2.0\n",
      "  Downloading MarkupSafe-2.1.5-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26 kB)\n",
      "Collecting urllib3<3,>=1.21.1\n",
      "  Downloading urllib3-2.2.1-py3-none-any.whl (121 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 121.1/121.1 kB 22.0 MB/s eta 0:00:00\n",
      "Collecting charset-normalizer<4,>=2\n",
      "  Downloading charset_normalizer-3.3.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 141.1/141.1 kB 23.1 MB/s eta 0:00:00\n",
      "Collecting idna<4,>=2.5\n",
      "  Downloading idna-3.7-py3-none-any.whl (66 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 66.8/66.8 kB 11.8 MB/s eta 0:00:00\n",
      "Collecting certifi>=2017.4.17\n",
      "  Downloading certifi-2024.2.2-py3-none-any.whl (163 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 163.8/163.8 kB 28.6 MB/s eta 0:00:00\n",
      "Collecting mpmath>=0.19\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 536.2/536.2 kB 58.4 MB/s eta 0:00:00\n",
      "Installing collected packages: mpmath, urllib3, typing-extensions, tqdm, sympy, safetensors, regex, pyyaml, packaging, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, networkx, MarkupSafe, idna, fsspec, filelock, charset-normalizer, certifi, triton, requests, nvidia-cusparse-cu12, nvidia-cudnn-cu12, jinja2, nvidia-cusolver-cu12, huggingface-hub, torch, tokenizers, transformers\n",
      "Successfully installed MarkupSafe-2.1.5 certifi-2024.2.2 charset-normalizer-3.3.2 filelock-3.14.0 fsspec-2024.3.1 huggingface-hub-0.23.0 idna-3.7 jinja2-3.1.3 mpmath-1.3.0 networkx-3.1 numpy-1.24.4 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105 packaging-24.0 pyyaml-6.0.1 regex-2024.4.28 requests-2.31.0 safetensors-0.4.3 sympy-1.12 tokenizers-0.15.2 torch-2.2.0 tqdm-4.66.4 transformers-4.38.1 triton-2.2.0 typing-extensions-4.11.0 urllib3-2.2.1\n",
      "\u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0m\u001b[91m\n",
      "[notice] A new release of pip is available: 23.0.1 -> 24.0\n",
      "[notice] To update, run: pip install --upgrade pip\n",
      "\u001b[0mRemoving intermediate container 422e60722f89\n",
      " ---> 83a6dc2d77ab\n",
      "Step 5/6 : RUN python -c \"from transformers import pipeline; classifier = pipeline('text-classification', model='./model')\"\n",
      " ---> Running in 4f3f068e971a\n",
      "\u001b[91mTraceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.8/site-packages/transformers/utils/hub.py\", line 398, in cached_file\n",
      "    resolved_file = hf_hub_download(\n",
      "  File \"/usr/local/lib/python3.8/site-packages/huggingface_hub/utils/_validators.py\", line 106, in _inner_fn\n",
      "    validate_repo_id(arg_value)\n",
      "  File \"/usr/local/lib/python3.8/site-packages/huggingface_hub/utils/_validators.py\", line 160, in validate_repo_id\n",
      "    raise HFValidationError(\n",
      "huggingface_hub.errors.HFValidationError: Repo id must use alphanumeric chars or '-', '_', '.', '--' and '..' are forbidden, '-' and '.' cannot start or end the name, max length is 96: './model'.\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/usr/local/lib/python3.8/site-packages/transformers/pipelines/__init__.py\", line 779, in pipeline\n",
      "    resolved_config_file = cached_file(\n",
      "  File \"/usr/local/lib/python3.8/site-packages/transformers/utils/hub.py\", line 462, in cached_file\n",
      "    raise EnvironmentError(\n",
      "OSError: Incorrect path_or_model_id: './model'. Please provide either the path to a local folder or the repo_id of a model on the Hub.\n",
      "\u001b[0mThe command '/bin/sh -c python -c \"from transformers import pipeline; classifier = pipeline('text-classification', model='./model')\"' returned a non-zero code: 1\n",
      "The push refers to repository [us-central1-docker.pkg.dev/tidy-reporter-417118/huggingface-models/bert-phishing]\n",
      "\n",
      "\u001b[1Bc5d62699: Preparing \n",
      "\u001b[1B40da2092: Preparing \n",
      "\u001b[1B1f471434: Preparing \n",
      "\u001b[1B7f754ef8: Preparing \n",
      "\u001b[1Bbec46633: Preparing \n",
      "\u001b[1Bd57a5024: Preparing \n",
      "\u001b[2Bd57a5024: Layer already exists \u001b[4A\u001b[2Klatest: digest: sha256:1853473d109a31135879150894f78388225b08029f4ebe422db4063b489eed57 size: 1798\n",
      "Creating Model\n",
      "Create Model backing LRO: projects/864682987145/locations/us-central1/models/3077454980819451904/operations/4032425394123571200\n",
      "Model created. Resource name: projects/864682987145/locations/us-central1/models/3077454980819451904@1\n",
      "To use this Model in another session:\n",
      "model = aiplatform.Model('projects/864682987145/locations/us-central1/models/3077454980819451904@1')\n",
      "Creating Endpoint\n",
      "Create Endpoint backing LRO: projects/864682987145/locations/us-central1/endpoints/5666062693883183104/operations/4329662969530023936\n",
      "Endpoint created. Resource name: projects/864682987145/locations/us-central1/endpoints/5666062693883183104\n",
      "To use this Endpoint in another session:\n",
      "endpoint = aiplatform.Endpoint('projects/864682987145/locations/us-central1/endpoints/5666062693883183104')\n",
      "Deploying model to Endpoint : projects/864682987145/locations/us-central1/endpoints/5666062693883183104\n",
      "Deploy Endpoint model backing LRO: projects/864682987145/locations/us-central1/endpoints/5666062693883183104/operations/7750146886517915648\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 59\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m# Upload and deploy the model directly using the image\u001b[39;00m\n\u001b[1;32m     54\u001b[0m model \u001b[38;5;241m=\u001b[39m aiplatform\u001b[38;5;241m.\u001b[39mModel\u001b[38;5;241m.\u001b[39mupload(\n\u001b[1;32m     55\u001b[0m     display_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbert-phishing\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     56\u001b[0m     serving_container_image_uri\u001b[38;5;241m=\u001b[39mimage_uri\n\u001b[1;32m     57\u001b[0m )\n\u001b[0;32m---> 59\u001b[0m endpoint \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeploy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmachine_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn1-standard-4\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccelerator_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mNVIDIA_TESLA_T4\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccelerator_count\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[1;32m     63\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel deployed. Endpoint:\u001b[39m\u001b[38;5;124m\"\u001b[39m, endpoint\u001b[38;5;241m.\u001b[39mresource_name)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/google/cloud/aiplatform/models.py:3620\u001b[0m, in \u001b[0;36mModel.deploy\u001b[0;34m(self, endpoint, deployed_model_display_name, traffic_percentage, traffic_split, machine_type, min_replica_count, max_replica_count, accelerator_type, accelerator_count, tpu_topology, service_account, explanation_metadata, explanation_parameters, metadata, encryption_spec_key_name, network, sync, deploy_request_timeout, autoscaling_target_cpu_utilization, autoscaling_target_accelerator_duty_cycle, enable_access_logging, disable_container_logging)\u001b[0m\n\u001b[1;32m   3609\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   3610\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraffic splitting is not yet supported for PrivateEndpoint. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3611\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTry calling deploy() without providing `traffic_split`. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3612\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA maximum of one model can be deployed to each private Endpoint.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3613\u001b[0m         )\n\u001b[1;32m   3615\u001b[0m explanation_spec \u001b[38;5;241m=\u001b[39m _explanation_utils\u001b[38;5;241m.\u001b[39mcreate_and_validate_explanation_spec(\n\u001b[1;32m   3616\u001b[0m     explanation_metadata\u001b[38;5;241m=\u001b[39mexplanation_metadata,\n\u001b[1;32m   3617\u001b[0m     explanation_parameters\u001b[38;5;241m=\u001b[39mexplanation_parameters,\n\u001b[1;32m   3618\u001b[0m )\n\u001b[0;32m-> 3620\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_deploy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3621\u001b[0m \u001b[43m    \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3622\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdeployed_model_display_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdeployed_model_display_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3623\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtraffic_percentage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraffic_percentage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3624\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtraffic_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraffic_split\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3625\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmachine_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmachine_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3626\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmin_replica_count\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmin_replica_count\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3627\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_replica_count\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_replica_count\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3628\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccelerator_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccelerator_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3629\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccelerator_count\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccelerator_count\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3630\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtpu_topology\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtpu_topology\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3631\u001b[0m \u001b[43m    \u001b[49m\u001b[43mservice_account\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mservice_account\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3632\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexplanation_spec\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexplanation_spec\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3633\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3634\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencryption_spec_key_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencryption_spec_key_name\u001b[49m\n\u001b[1;32m   3635\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minitializer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mglobal_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencryption_spec_key_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3636\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnetwork\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnetwork\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3637\u001b[0m \u001b[43m    \u001b[49m\u001b[43msync\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msync\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3638\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdeploy_request_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdeploy_request_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3639\u001b[0m \u001b[43m    \u001b[49m\u001b[43mautoscaling_target_cpu_utilization\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mautoscaling_target_cpu_utilization\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3640\u001b[0m \u001b[43m    \u001b[49m\u001b[43mautoscaling_target_accelerator_duty_cycle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mautoscaling_target_accelerator_duty_cycle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3641\u001b[0m \u001b[43m    \u001b[49m\u001b[43menable_access_logging\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menable_access_logging\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3642\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdisable_container_logging\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisable_container_logging\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3643\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/google/cloud/aiplatform/base.py:863\u001b[0m, in \u001b[0;36moptional_sync.<locals>.optional_run_in_thread.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    861\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m    862\u001b[0m         VertexAiResourceNounWithFutureManager\u001b[38;5;241m.\u001b[39mwait(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 863\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    865\u001b[0m \u001b[38;5;66;03m# callbacks to call within the Future (in same Thread)\u001b[39;00m\n\u001b[1;32m    866\u001b[0m internal_callbacks \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/google/cloud/aiplatform/models.py:3797\u001b[0m, in \u001b[0;36mModel._deploy\u001b[0;34m(self, endpoint, deployed_model_display_name, traffic_percentage, traffic_split, machine_type, min_replica_count, max_replica_count, accelerator_type, accelerator_count, tpu_topology, service_account, explanation_spec, metadata, encryption_spec_key_name, network, sync, deploy_request_timeout, autoscaling_target_cpu_utilization, autoscaling_target_accelerator_duty_cycle, enable_access_logging, disable_container_logging)\u001b[0m\n\u001b[1;32m   3786\u001b[0m         endpoint \u001b[38;5;241m=\u001b[39m PrivateEndpoint\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[1;32m   3787\u001b[0m             display_name\u001b[38;5;241m=\u001b[39mdisplay_name,\n\u001b[1;32m   3788\u001b[0m             network\u001b[38;5;241m=\u001b[39mnetwork,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3792\u001b[0m             encryption_spec_key_name\u001b[38;5;241m=\u001b[39mencryption_spec_key_name,\n\u001b[1;32m   3793\u001b[0m         )\n\u001b[1;32m   3795\u001b[0m _LOGGER\u001b[38;5;241m.\u001b[39mlog_action_start_against_resource(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDeploying model to\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m, endpoint)\n\u001b[0;32m-> 3797\u001b[0m \u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_deploy_call\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapi_client\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresource_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3800\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3801\u001b[0m \u001b[43m    \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gca_resource\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraffic_split\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3802\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnetwork\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnetwork\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnetwork\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3803\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdeployed_model_display_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdeployed_model_display_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3804\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtraffic_percentage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraffic_percentage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3805\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtraffic_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraffic_split\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3806\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmachine_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmachine_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3807\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmin_replica_count\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmin_replica_count\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3808\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_replica_count\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_replica_count\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3809\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccelerator_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccelerator_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3810\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccelerator_count\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccelerator_count\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3811\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtpu_topology\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtpu_topology\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3812\u001b[0m \u001b[43m    \u001b[49m\u001b[43mservice_account\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mservice_account\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3813\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexplanation_spec\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexplanation_spec\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3814\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3815\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdeploy_request_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdeploy_request_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3816\u001b[0m \u001b[43m    \u001b[49m\u001b[43mautoscaling_target_cpu_utilization\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mautoscaling_target_cpu_utilization\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3817\u001b[0m \u001b[43m    \u001b[49m\u001b[43mautoscaling_target_accelerator_duty_cycle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mautoscaling_target_accelerator_duty_cycle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3818\u001b[0m \u001b[43m    \u001b[49m\u001b[43menable_access_logging\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menable_access_logging\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3819\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdisable_container_logging\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisable_container_logging\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3820\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3822\u001b[0m _LOGGER\u001b[38;5;241m.\u001b[39mlog_action_completed_against_resource(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdeployed\u001b[39m\u001b[38;5;124m\"\u001b[39m, endpoint)\n\u001b[1;32m   3824\u001b[0m endpoint\u001b[38;5;241m.\u001b[39m_sync_gca_resource()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/google/cloud/aiplatform/models.py:1319\u001b[0m, in \u001b[0;36mEndpoint._deploy_call\u001b[0;34m(cls, api_client, endpoint_resource_name, model, endpoint_resource_traffic_split, network, deployed_model_display_name, traffic_percentage, traffic_split, machine_type, min_replica_count, max_replica_count, accelerator_type, accelerator_count, tpu_topology, service_account, explanation_spec, metadata, deploy_request_timeout, autoscaling_target_cpu_utilization, autoscaling_target_accelerator_duty_cycle, enable_access_logging, disable_container_logging)\u001b[0m\n\u001b[1;32m   1307\u001b[0m operation_future \u001b[38;5;241m=\u001b[39m api_client\u001b[38;5;241m.\u001b[39mdeploy_model(\n\u001b[1;32m   1308\u001b[0m     endpoint\u001b[38;5;241m=\u001b[39mendpoint_resource_name,\n\u001b[1;32m   1309\u001b[0m     deployed_model\u001b[38;5;241m=\u001b[39mdeployed_model,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1312\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mdeploy_request_timeout,\n\u001b[1;32m   1313\u001b[0m )\n\u001b[1;32m   1315\u001b[0m _LOGGER\u001b[38;5;241m.\u001b[39mlog_action_started_against_resource_with_lro(\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDeploy\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mcls\u001b[39m, operation_future\n\u001b[1;32m   1317\u001b[0m )\n\u001b[0;32m-> 1319\u001b[0m \u001b[43moperation_future\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/google/api_core/future/polling.py:256\u001b[0m, in \u001b[0;36mPollingFuture.result\u001b[0;34m(self, timeout, retry, polling)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mresult\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m_DEFAULT_VALUE, retry\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, polling\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    145\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get the result of the operation.\u001b[39;00m\n\u001b[1;32m    146\u001b[0m \n\u001b[1;32m    147\u001b[0m \u001b[38;5;124;03m    This method will poll for operation status periodically, blocking if\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[38;5;124;03m            the timeout is reached before the operation completes.\u001b[39;00m\n\u001b[1;32m    254\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 256\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_blocking_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretry\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpolling\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpolling\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    258\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    259\u001b[0m         \u001b[38;5;66;03m# pylint: disable=raising-bad-type\u001b[39;00m\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;66;03m# Pylint doesn't recognize that this is valid in this case.\u001b[39;00m\n\u001b[1;32m    261\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/google/api_core/future/polling.py:137\u001b[0m, in \u001b[0;36mPollingFuture._blocking_poll\u001b[0;34m(self, timeout, retry, polling)\u001b[0m\n\u001b[1;32m    134\u001b[0m     polling \u001b[38;5;241m=\u001b[39m polling\u001b[38;5;241m.\u001b[39mwith_timeout(timeout)\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 137\u001b[0m     \u001b[43mpolling\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_done_or_raise\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretry\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mRetryError:\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m concurrent\u001b[38;5;241m.\u001b[39mfutures\u001b[38;5;241m.\u001b[39mTimeoutError(\n\u001b[1;32m    140\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOperation did not complete within the designated timeout of \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    141\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpolling\u001b[38;5;241m.\u001b[39mtimeout\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    142\u001b[0m     )\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/google/api_core/retry.py:349\u001b[0m, in \u001b[0;36mRetry.__call__.<locals>.retry_wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    345\u001b[0m target \u001b[38;5;241m=\u001b[39m functools\u001b[38;5;241m.\u001b[39mpartial(func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    346\u001b[0m sleep_generator \u001b[38;5;241m=\u001b[39m exponential_sleep_generator(\n\u001b[1;32m    347\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initial, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maximum, multiplier\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_multiplier\n\u001b[1;32m    348\u001b[0m )\n\u001b[0;32m--> 349\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mretry_target\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_predicate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43msleep_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43mon_error\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mon_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/google/api_core/retry.py:217\u001b[0m, in \u001b[0;36mretry_target\u001b[0;34m(target, predicate, sleep_generator, timeout, on_error, **kwargs)\u001b[0m\n\u001b[1;32m    207\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mRetryError(\n\u001b[1;32m    208\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDeadline of \u001b[39m\u001b[38;5;132;01m{:.1f}\u001b[39;00m\u001b[38;5;124ms exceeded while calling target function\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m    209\u001b[0m                     timeout\n\u001b[1;32m    210\u001b[0m                 ),\n\u001b[1;32m    211\u001b[0m                 last_exc,\n\u001b[1;32m    212\u001b[0m             ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlast_exc\u001b[39;00m\n\u001b[1;32m    214\u001b[0m     _LOGGER\u001b[38;5;241m.\u001b[39mdebug(\n\u001b[1;32m    215\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRetrying due to \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, sleeping \u001b[39m\u001b[38;5;132;01m{:.1f}\u001b[39;00m\u001b[38;5;124ms ...\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(last_exc, sleep)\n\u001b[1;32m    216\u001b[0m     )\n\u001b[0;32m--> 217\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43msleep\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSleep generator stopped yielding sleep values.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Install necessary libraries\n",
    "!pip install transformers torch google-cloud-aiplatform tqdm git-lfs\n",
    "\n",
    "# Import necessary libraries\n",
    "import os\n",
    "import tarfile\n",
    "from google.cloud import aiplatform\n",
    "\n",
    "# Set Google Cloud settings\n",
    "PROJECT_ID = \"tidy-reporter-417118\"\n",
    "REGION = \"us-central1\"\n",
    "REPOSITORY = \"huggingface-models\"\n",
    "IMAGE = \"bert-phishing\"\n",
    "TAG = \"latest\"\n",
    "\n",
    "# Authenticate gcloud and configure Docker non-interactively\n",
    "!gcloud config set project {PROJECT_ID}\n",
    "!gcloud auth configure-docker {REGION}-docker.pkg.dev --quiet\n",
    "\n",
    "# Install Git LFS\n",
    "!sudo apt-get install git-lfs\n",
    "!git lfs install --skip-smudge\n",
    "\n",
    "# Clone the Hugging Face model repository\n",
    "!rm -rf huggingface_model\n",
    "!git clone https://huggingface.co/ealvaradob/bert-finetuned-phishing huggingface_model\n",
    "\n",
    "# Change directory to where the model is cloned\n",
    "os.chdir('huggingface_model')\n",
    "\n",
    "# Prepare the Dockerfile to include the model in the image\n",
    "dockerfile_contents = \"\"\"\n",
    "FROM python:3.8-slim\n",
    "WORKDIR /\n",
    "COPY . /\n",
    "RUN pip install transformers==4.38.1 torch==2.2.0\n",
    "RUN python -c \"from transformers import pipeline; classifier = pipeline('text-classification', model='./model')\"\n",
    "ENTRYPOINT [\"python\", \"predictor.py\"]\n",
    "\"\"\"\n",
    "\n",
    "# Write the Dockerfile\n",
    "with open(\"Dockerfile\", \"w\") as f:\n",
    "    f.write(dockerfile_contents)\n",
    "\n",
    "# Build and push the Docker image with the model included\n",
    "image_uri = f\"{REGION}-docker.pkg.dev/{PROJECT_ID}/{REPOSITORY}/{IMAGE}:{TAG}\"\n",
    "!docker build -t {image_uri} .\n",
    "!docker push {image_uri}\n",
    "\n",
    "# Initialize AI platform\n",
    "aiplatform.init(project=PROJECT_ID, location=REGION)\n",
    "\n",
    "# Upload and deploy the model directly using the image\n",
    "model = aiplatform.Model.upload(\n",
    "    display_name=\"bert-phishing\",\n",
    "    serving_container_image_uri=image_uri\n",
    ")\n",
    "\n",
    "endpoint = model.deploy(\n",
    "    machine_type=\"n1-standard-4\",\n",
    "    accelerator_type=\"NVIDIA_TESLA_T4\",\n",
    "    accelerator_count=1\n",
    ")\n",
    "\n",
    "print(\"Model deployed. Endpoint:\", endpoint.resource_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "77311f25-4553-4aef-b209-c6993800780e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file://model.tar.gz [Content-Type=application/x-tar]...\n",
      "/ [1 files][316.0 KiB/316.0 KiB]                                                \n",
      "Operation completed over 1 objects/316.0 KiB.                                    \n",
      "gs://tidy-reporter-model-bucket/model.tar.gz\n",
      "gs://tidy-reporter-model-bucket/model.tar.gz\n"
     ]
    }
   ],
   "source": [
    "# Attempt to re-upload the model.tar.gz file\n",
    "!gsutil cp model.tar.gz gs://{BUCKET_NAME}/\n",
    "\n",
    "# List files in the bucket to confirm upload\n",
    "!gsutil ls gs://{BUCKET_NAME}/\n",
    "!gsutil ls gs://{BUCKET_NAME}/model.tar.gz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "54ef9689-ce15-4d4a-ad7d-ae8b077b08e9",
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFound",
     "evalue": "404 There are no files in directory \"gs://tidy-reporter-model-bucket/model.tar.gz\". Please check if the Cloud Storage URI is correct or copy at least one file to the directory.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31m_InactiveRpcError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/google/api_core/grpc_helpers.py:65\u001b[0m, in \u001b[0;36m_wrap_unary_errors.<locals>.error_remapped_callable\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcallable_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m grpc\u001b[38;5;241m.\u001b[39mRpcError \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/grpc/_channel.py:1176\u001b[0m, in \u001b[0;36m_UnaryUnaryMultiCallable.__call__\u001b[0;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[0m\n\u001b[1;32m   1170\u001b[0m (\n\u001b[1;32m   1171\u001b[0m     state,\n\u001b[1;32m   1172\u001b[0m     call,\n\u001b[1;32m   1173\u001b[0m ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_blocking(\n\u001b[1;32m   1174\u001b[0m     request, timeout, metadata, credentials, wait_for_ready, compression\n\u001b[1;32m   1175\u001b[0m )\n\u001b[0;32m-> 1176\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_end_unary_response_blocking\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcall\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/grpc/_channel.py:1005\u001b[0m, in \u001b[0;36m_end_unary_response_blocking\u001b[0;34m(state, call, with_call, deadline)\u001b[0m\n\u001b[1;32m   1004\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1005\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m _InactiveRpcError(state)\n",
      "\u001b[0;31m_InactiveRpcError\u001b[0m: <_InactiveRpcError of RPC that terminated with:\n\tstatus = StatusCode.NOT_FOUND\n\tdetails = \"There are no files in directory \"gs://tidy-reporter-model-bucket/model.tar.gz\". Please check if the Cloud Storage URI is correct or copy at least one file to the directory.\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv4:64.233.181.95:443 {created_time:\"2024-05-05T21:09:45.901654795+00:00\", grpc_status:5, grpc_message:\"There are no files in directory \\\"gs://tidy-reporter-model-bucket/model.tar.gz\\\". Please check if the Cloud Storage URI is correct or copy at least one file to the directory.\"}\"\n>",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mNotFound\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m aiplatform\u001b[38;5;241m.\u001b[39minit(project\u001b[38;5;241m=\u001b[39mPROJECT_ID, location\u001b[38;5;241m=\u001b[39mREGION)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Upload and deploy the model\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43maiplatform\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupload\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdisplay_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbert-phishing\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43martifact_uri\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgs://\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mBUCKET_NAME\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/model.tar.gz\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mserving_container_image_uri\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mREGION\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m-docker.pkg.dev/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mPROJECT_ID\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mREPOSITORY\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mIMAGE\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m:\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mTAG\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m      9\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m endpoint \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mdeploy(\n\u001b[1;32m     12\u001b[0m     machine_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn1-standard-4\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     13\u001b[0m     accelerator_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNVIDIA_TESLA_T4\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     14\u001b[0m     accelerator_count\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     15\u001b[0m )\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel deployed. Endpoint:\u001b[39m\u001b[38;5;124m\"\u001b[39m, endpoint\u001b[38;5;241m.\u001b[39mresource_name)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/google/cloud/aiplatform/base.py:863\u001b[0m, in \u001b[0;36moptional_sync.<locals>.optional_run_in_thread.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    861\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m    862\u001b[0m         VertexAiResourceNounWithFutureManager\u001b[38;5;241m.\u001b[39mwait(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 863\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    865\u001b[0m \u001b[38;5;66;03m# callbacks to call within the Future (in same Thread)\u001b[39;00m\n\u001b[1;32m    866\u001b[0m internal_callbacks \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/google/cloud/aiplatform/models.py:3439\u001b[0m, in \u001b[0;36mModel.upload\u001b[0;34m(cls, serving_container_image_uri, artifact_uri, model_id, parent_model, is_default_version, version_aliases, version_description, serving_container_predict_route, serving_container_health_route, description, serving_container_command, serving_container_args, serving_container_environment_variables, serving_container_ports, serving_container_grpc_ports, local_model, instance_schema_uri, parameters_schema_uri, prediction_schema_uri, explanation_metadata, explanation_parameters, display_name, project, location, credentials, labels, encryption_spec_key_name, staging_bucket, sync, upload_request_timeout, serving_container_deployment_timeout, serving_container_shared_memory_size_mb, serving_container_startup_probe_exec, serving_container_startup_probe_period_seconds, serving_container_startup_probe_timeout_seconds, serving_container_health_probe_exec, serving_container_health_probe_period_seconds, serving_container_health_probe_timeout_seconds)\u001b[0m\n\u001b[1;32m   3428\u001b[0m request \u001b[38;5;241m=\u001b[39m gca_model_service_compat\u001b[38;5;241m.\u001b[39mUploadModelRequest(\n\u001b[1;32m   3429\u001b[0m     parent\u001b[38;5;241m=\u001b[39minitializer\u001b[38;5;241m.\u001b[39mglobal_config\u001b[38;5;241m.\u001b[39mcommon_location_path(project, location),\n\u001b[1;32m   3430\u001b[0m     model\u001b[38;5;241m=\u001b[39mmanaged_model,\n\u001b[1;32m   3431\u001b[0m     parent_model\u001b[38;5;241m=\u001b[39mparent_model,\n\u001b[1;32m   3432\u001b[0m     model_id\u001b[38;5;241m=\u001b[39mmodel_id,\n\u001b[1;32m   3433\u001b[0m )\n\u001b[1;32m   3435\u001b[0m api_client \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_instantiate_client(\n\u001b[1;32m   3436\u001b[0m     location, credentials, appended_user_agent\u001b[38;5;241m=\u001b[39mappended_user_agent\n\u001b[1;32m   3437\u001b[0m )\n\u001b[0;32m-> 3439\u001b[0m lro \u001b[38;5;241m=\u001b[39m \u001b[43mapi_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupload_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3440\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3441\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mupload_request_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3442\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3444\u001b[0m _LOGGER\u001b[38;5;241m.\u001b[39mlog_create_with_lro(\u001b[38;5;28mcls\u001b[39m, lro)\n\u001b[1;32m   3446\u001b[0m model_upload_response \u001b[38;5;241m=\u001b[39m lro\u001b[38;5;241m.\u001b[39mresult()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/google/cloud/aiplatform_v1/services/model_service/client.py:916\u001b[0m, in \u001b[0;36mModelServiceClient.upload_model\u001b[0;34m(self, request, parent, model, retry, timeout, metadata)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_universe_domain()\n\u001b[1;32m    915\u001b[0m \u001b[38;5;66;03m# Send the request.\u001b[39;00m\n\u001b[0;32m--> 916\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mrpc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    917\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    918\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretry\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretry\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    919\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    920\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    921\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    923\u001b[0m \u001b[38;5;66;03m# Wrap the response in an operation future.\u001b[39;00m\n\u001b[1;32m    924\u001b[0m response \u001b[38;5;241m=\u001b[39m gac_operation\u001b[38;5;241m.\u001b[39mfrom_gapic(\n\u001b[1;32m    925\u001b[0m     response,\n\u001b[1;32m    926\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transport\u001b[38;5;241m.\u001b[39moperations_client,\n\u001b[1;32m    927\u001b[0m     model_service\u001b[38;5;241m.\u001b[39mUploadModelResponse,\n\u001b[1;32m    928\u001b[0m     metadata_type\u001b[38;5;241m=\u001b[39mmodel_service\u001b[38;5;241m.\u001b[39mUploadModelOperationMetadata,\n\u001b[1;32m    929\u001b[0m )\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/google/api_core/gapic_v1/method.py:113\u001b[0m, in \u001b[0;36m_GapicCallable.__call__\u001b[0;34m(self, timeout, retry, *args, **kwargs)\u001b[0m\n\u001b[1;32m    110\u001b[0m     metadata\u001b[38;5;241m.\u001b[39mextend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_metadata)\n\u001b[1;32m    111\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m metadata\n\u001b[0;32m--> 113\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/pytorch/lib/python3.10/site-packages/google/api_core/grpc_helpers.py:67\u001b[0m, in \u001b[0;36m_wrap_unary_errors.<locals>.error_remapped_callable\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m callable_(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m grpc\u001b[38;5;241m.\u001b[39mRpcError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m---> 67\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mfrom_grpc_error(exc) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexc\u001b[39;00m\n",
      "\u001b[0;31mNotFound\u001b[0m: 404 There are no files in directory \"gs://tidy-reporter-model-bucket/model.tar.gz\". Please check if the Cloud Storage URI is correct or copy at least one file to the directory."
     ]
    }
   ],
   "source": [
    "# Initialize AI platform with the correct project and location\n",
    "aiplatform.init(project=PROJECT_ID, location=REGION)\n",
    "\n",
    "# Upload and deploy the model\n",
    "model = aiplatform.Model.upload(\n",
    "    display_name=\"bert-phishing\",\n",
    "    artifact_uri=f\"gs://{BUCKET_NAME}/model.tar.gz\",\n",
    "    serving_container_image_uri=f\"{REGION}-docker.pkg.dev/{PROJECT_ID}/{REPOSITORY}/{IMAGE}:{TAG}\"\n",
    ")\n",
    "\n",
    "endpoint = model.deploy(\n",
    "    machine_type=\"n1-standard-4\",\n",
    "    accelerator_type=\"NVIDIA_TESLA_T4\",\n",
    "    accelerator_count=1\n",
    ")\n",
    "\n",
    "print(\"Model deployed. Endpoint:\", endpoint.resource_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a62cee2-8b82-40c2-b06f-c9e5b90cedc1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-env-pytorch-pytorch",
   "name": "workbench-notebooks.m120",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m120"
  },
  "kernelspec": {
   "display_name": "PyTorch 1-13",
   "language": "python",
   "name": "conda-env-pytorch-pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
